def pascal_triangle(n):
    for i in range(n):
        num = 1
        row = ""
        for j in range(i + 1):
            row += str(num)
            num = num * (i - j) // (j + 1)
        # Pad with trailing zeroes to make all rows the same length
        row = row.ljust(n, '0')
        print(row)

# Input from user
N = int(input("Enter number of rows: "))
pascal_triangle(N)








import json
import yaml
import psutil
import requests
from bs4 import BeautifulSoup
import logging
from datetime import datetime

# Setup logging
logging.basicConfig(
    filename='status_report.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def load_json_config(filepath):
    try:
        with open(filepath) as f:
            config = json.load(f)
        logging.info(f"Loaded JSON config from {filepath}")
        return config
    except Exception as e:
        logging.error(f"Error loading JSON config: {e}")
        return {}

def load_yaml_config(filepath):
    try:
        with open(filepath) as f:
            config = yaml.safe_load(f)
        logging.info(f"Loaded YAML config from {filepath}")
        return config
    except Exception as e:
        logging.error(f"Error loading YAML config: {e}")
        return {}

def get_system_specs():
    try:
        specs = {
            "cpu_percent": psutil.cpu_percent(interval=1),
            "memory_percent": psutil.virtual_memory().percent,
            "disk_percent": psutil.disk_usage('/').percent
        }
        logging.info("Collected system specifications")
        return specs
    except Exception as e:
        logging.error(f"Error collecting system specs: {e}")
        return {}

def scrape_website(url):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
        data = {
            "http_status": response.status_code,
            "page_title": soup.title.string.strip() if soup.title else "N/A",
            "headings": [tag.get_text(strip=True) for tag in soup.find_all(['h1', 'h2', 'h3'])]
        }
        logging.info(f"Scraped website: {url}")
        return data
    except Exception as e:
        logging.error(f"Error scraping website {url}: {e}")
        return {}

def generate_status_report(json_path, yaml_path, website_url, output_file):
    report = {
        "timestamp": datetime.now().isoformat(),
        "deployment_config": load_json_config(json_path),
        "service_config": load_yaml_config(yaml_path),
        "system_specifications": get_system_specs(),
        "website_data": scrape_website(website_url)
    }

    try:
        with open(output_file, 'w') as f:
            json.dump(report, f, indent=4)
        logging.info(f"Status report saved to {output_file}")
        print(f"Report generated: {output_file}")
    except Exception as e:
        logging.error(f"Error writing report to {output_file}: {e}")

# ---- Main ----
if __name__ == "__main__":
    generate_status_report(
        json_path="config.json",
        yaml_path="config.yaml",
        website_url="https://www.python.org/",
        output_file="status_report.json"
    )

